{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer-Model_Eng_Bengali.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOWoT39b6mcYh7sF3mlZxLR"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"XfI2dWkSdSUi"},"source":["**Lannguage Translation from English to Bengali_INDIA using Transformer Model**"]},{"cell_type":"markdown","metadata":{"id":"ftSSWanUxbY4"},"source":["**1. Dependencies**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mamYJl4fdoQc","executionInfo":{"status":"ok","timestamp":1610373904919,"user_tz":-330,"elapsed":5883,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}},"outputId":"e70f0295-7455-4a06-e1f2-5de948a21262"},"source":["import numpy as np\r\n","import math\r\n","import re\r\n","import time\r\n","from google.colab import drive\r\n","\r\n","try:\r\n","  %tensorflow_version 2.x\r\n","except:\r\n","  pass\r\n","import tensorflow as tf\r\n","from tensorflow.keras import layers\r\n","import tensorflow_datasets as tfds\r\n","print(\"tensorflow version = \",tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["tensorflow version =  2.4.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"KfxB9Z6wxo_J"},"source":["**2.Mount google drive and read the preloaded data files**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h7V0LK_8rpZ3","executionInfo":{"status":"ok","timestamp":1610373929395,"user_tz":-330,"elapsed":30350,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}},"outputId":"c91d1243-2a23-4d09-e504-caf25c1fddbb"},"source":["drive.mount(\"/content/drive\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"S1ApURY5skFR","executionInfo":{"status":"ok","timestamp":1610373932755,"user_tz":-330,"elapsed":33705,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["with open(\"/content/drive/MyDrive/Colab Notebooks/Projects/data/GNOME.bn_IN-en.en\",\r\n","          mode = 'r',\r\n","          encoding=\"utf-8\") as f:\r\n","     opus_english = f.read()\r\n","\r\n","with open(\"/content/drive/MyDrive/Colab Notebooks/Projects/data/GNOME.bn_IN-en.bn_IN\",\r\n","          mode = 'r',\r\n","          encoding=\"utf-8\") as f:\r\n","     opus_bengali = f.read()\r\n","\r\n","with open(\"/content/drive/MyDrive/Colab Notebooks/Projects/data/nonbreaking_prefix.en.txt\",\r\n","          mode = 'r',\r\n","          encoding=\"utf-8\") as f:\r\n","     nonbreaking_prefix_en = f.read()\r\n","\r\n","with open(\"/content/drive/MyDrive/Colab Notebooks/Projects/data/nonbreaking_prefix.bn.txt\",\r\n","          mode = 'r',\r\n","          encoding=\"utf-8\") as f:\r\n","     nonbreaking_prefix_bn = f.read()\r\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oT47S_uewez0","executionInfo":{"status":"ok","timestamp":1610373932756,"user_tz":-330,"elapsed":33703,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}},"outputId":"9a3db659-8221-4e34-8bd7-b210e2ef6717"},"source":["print(opus_english[:60])\r\n","print(opus_bengali[:60])\r\n","print(nonbreaking_prefix_en[:5])\r\n","print(nonbreaking_prefix_bn[:5])\r\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Give your application an accessibility workout\n","Accerciser Ac\n","আপনার অ্যাপ্লিকেশনে বিশেষ ব্যবহারের ক্ষমতা উপলব্ধ করুন\n","Accer\n","A\n","B\n","C\n","ড\n","এ\n","ব\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"qFsEBkfax7fz"},"source":["**3. Clean the data**"]},{"cell_type":"markdown","metadata":{"id":"wfwtA7pawKrV"},"source":["Getting the non breaking prefixes as a clean list of words with a space at the beggining and a point at the end, for easier to use."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KdfvfJhIwq3O","executionInfo":{"status":"ok","timestamp":1610373932757,"user_tz":-330,"elapsed":33699,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}},"outputId":"7e37fd27-3004-4abf-c580-43de94fed309"},"source":["nonbreaking_prefix_en\r\n","nonbreaking_prefix_en = nonbreaking_prefix_en.split(\"\\n\")\r\n","nonbreaking_prefix_en = [' ' + pref + '.' for pref in nonbreaking_prefix_en]\r\n","nonbreaking_prefix_bn = nonbreaking_prefix_bn.split(\"\\n\")\r\n","nonbreaking_prefix_bn = [' ' + pref + '.' for pref in nonbreaking_prefix_bn]\r\n","nonbreaking_prefix_en"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[' A.',\n"," ' B.',\n"," ' C.',\n"," ' D.',\n"," ' E.',\n"," ' F.',\n"," ' G.',\n"," ' H.',\n"," ' I.',\n"," ' J.',\n"," ' K.',\n"," ' L.',\n"," ' M.',\n"," ' N.',\n"," ' O.',\n"," ' P.',\n"," ' Q.',\n"," ' R.',\n"," ' S.',\n"," ' T.',\n"," ' U.',\n"," ' V.',\n"," ' W.',\n"," ' X.',\n"," ' Y.',\n"," ' Z.',\n"," ' Adj.',\n"," ' Adm.',\n"," ' Adv.',\n"," ' Asst.',\n"," ' Bart.',\n"," ' Bldg.',\n"," ' Brig.',\n"," ' Bros.',\n"," ' Capt.',\n"," ' Cmdr.',\n"," ' Col.',\n"," ' Comdr.',\n"," ' Con.',\n"," ' Corp.',\n"," ' Cpl.',\n"," ' DR.',\n"," ' Dr.',\n"," ' Drs.',\n"," ' Ens.',\n"," ' Gen.',\n"," ' Gov.',\n"," ' Hon.',\n"," ' Hr.',\n"," ' Hosp.',\n"," ' Insp.',\n"," ' Lt.',\n"," ' MM.',\n"," ' MR.',\n"," ' MRS.',\n"," ' MS.',\n"," ' Maj.',\n"," ' Messrs.',\n"," ' Mlle.',\n"," ' Mme.',\n"," ' Mr.',\n"," ' Mrs.',\n"," ' Ms.',\n"," ' Msgr.',\n"," ' Op.',\n"," ' Ord.',\n"," ' Pfc.',\n"," ' Ph.',\n"," ' Prof.',\n"," ' Pvt.',\n"," ' Rep.',\n"," ' Reps.',\n"," ' Res.',\n"," ' Rev.',\n"," ' Rt.',\n"," ' Sen.',\n"," ' Sens.',\n"," ' Sfc.',\n"," ' Sgt.',\n"," ' Sr.',\n"," ' St.',\n"," ' Supt.',\n"," ' Surg.',\n"," ' v.',\n"," ' vs.',\n"," ' i.e.',\n"," ' rev.',\n"," ' e.g.',\n"," ' Rs.',\n"," ' No .',\n"," ' Nos.',\n"," ' Art.',\n"," ' Nr.',\n"," ' pp.',\n"," ' Jan.',\n"," ' Feb.',\n"," ' Mar.',\n"," ' Apr.',\n"," ' Jun.',\n"," ' Jul.',\n"," ' Aug.',\n"," ' Sep.',\n"," ' Oct.',\n"," ' Nov.',\n"," ' Dec.']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"kyoznMKOZDsT"},"source":["**Remove unwanted things**"]},{"cell_type":"code","metadata":{"id":"-UYkw1sy2I1H","executionInfo":{"status":"ok","timestamp":1610373934484,"user_tz":-330,"elapsed":35421,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["corpus_en = opus_english\r\n","for prefix in nonbreaking_prefix_en:\r\n","  corpus_en = corpus_en.replace(prefix, prefix + \"###\")\r\n","corpus_en = re.sub(\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_en)\r\n","corpus_en = re.sub(\"\\.###\", '', corpus_en)\r\n","corpus_en = re.sub(\"  +\", ' ',corpus_en)\r\n","corpus_en = corpus_en.split(\"\\n\")\r\n","\r\n","corpus_bn = opus_bengali\r\n","for prefix in nonbreaking_prefix_bn:\r\n","  corpus_bn = corpus_bn.replace(prefix, prefix + \"###\")\r\n","corpus_bn = re.sub(\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".###\", corpus_bn)\r\n","corpus_bn = re.sub(\"\\.###\", '', corpus_bn)\r\n","corpus_bn = re.sub(\"  +\", ' ',corpus_bn)\r\n","corpus_bn = corpus_bn.split(\"\\n\")"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"loJ0Fgn662VG"},"source":["**Tokenizing text**"]},{"cell_type":"code","metadata":{"id":"tds_oawb6-NO","executionInfo":{"status":"ok","timestamp":1610373984048,"user_tz":-330,"elapsed":84982,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**10)\r\n","tokenizer_bn = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_bn, target_vocab_size=2**10)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"frepASAFKCBA","executionInfo":{"status":"ok","timestamp":1610373984049,"user_tz":-330,"elapsed":84979,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2\r\n","VOCAB_SIZE_BN = tokenizer_bn.vocab_size + 2"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zddxXh9CYNtJ"},"source":["**Input Output Creation**"]},{"cell_type":"code","metadata":{"id":"wEaWE29GLKI7","executionInfo":{"status":"ok","timestamp":1610374002945,"user_tz":-330,"elapsed":103872,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["inputs = [[VOCAB_SIZE_EN - 2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN - 1] for sentence in corpus_en ]\r\n","\r\n","outputs = [[VOCAB_SIZE_BN - 2] + tokenizer_bn.encode(sentence) + [VOCAB_SIZE_BN - 1] for sentence in corpus_bn ]"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MGvcJgDmMhpQ"},"source":["**Remove too long sentenses**"]},{"cell_type":"code","metadata":{"id":"Sr1kRADOMwci","executionInfo":{"status":"ok","timestamp":1610374017100,"user_tz":-330,"elapsed":118024,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["MAX_LENGTH = 20\r\n","idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > MAX_LENGTH]\r\n","for idx in reversed(idx_to_remove): #we do not want extra processing, after one line deletion index changes, that's why reversed\r\n","  del inputs[idx]\r\n","  del outputs[idx]\r\n","\r\n","idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > MAX_LENGTH]\r\n","for idx in reversed(idx_to_remove): #we do not want extra processing, after one line deletion index changes, that's why reversed\r\n","  del inputs[idx]\r\n","  del outputs[idx]\r\n","\r\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kGMFNtjwezp5"},"source":["**Input/Output Creation - padding, shuffle etc**"]},{"cell_type":"markdown","metadata":{"id":"jPqbhpQ2GGFi"},"source":["As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."]},{"cell_type":"code","metadata":{"id":"LwM3wFxwfBwd","executionInfo":{"status":"ok","timestamp":1610374019687,"user_tz":-330,"elapsed":120608,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\r\n","                                                       value=0,\r\n","                                                       padding=\"post\",\r\n","                                                       maxlen=MAX_LENGTH)\r\n","outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\r\n","                                                       value=0,\r\n","                                                       padding=\"post\",\r\n","                                                       maxlen=MAX_LENGTH)"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"CcvuJkuDfyk1","executionInfo":{"status":"ok","timestamp":1610374019702,"user_tz":-330,"elapsed":120620,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["BATCH_SIZE = 64\r\n","BUFFER_SIZE = 20000\r\n","\r\n","dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\r\n","\r\n","dataset = dataset.cache()\r\n","dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\r\n","dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"58OdhX0zhFPr"},"source":["**Stage 3: ModelBuilding**"]},{"cell_type":"markdown","metadata":{"id":"3FO0iC2-hwEF"},"source":["**Embedding**"]},{"cell_type":"code","metadata":{"id":"f1Gs_1UThdEl","executionInfo":{"status":"ok","timestamp":1610374019703,"user_tz":-330,"elapsed":120618,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["class PositionalEncoding(layers.Layer):\r\n","  def __init__(self):\r\n","    super(PositionalEncoding, self).__init__()\r\n","\r\n","  def get_angles(self, pos, i, d_model):  #pos : (seq_length, 1) i: (1, d_model)\r\n","    angles = 1/ np.power(10000.0, (2*(i//2))/np.float32(d_model))\r\n","    return pos * angles #(seq_length, d_model)\r\n","\r\n","  def call(self, inputs):\r\n","        seq_length = inputs.shape.as_list()[-2]\r\n","        d_model = inputs.shape.as_list()[-1]\r\n","        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\r\n","                                 np.arange(d_model)[np.newaxis, :],\r\n","                                 d_model)\r\n","        angles[:, 0::2] = np.sin(angles[:, 0::2])\r\n","        angles[:, 1::2] = np.cos(angles[:, 1::2])\r\n","        pos_encoding = angles[np.newaxis, ...]\r\n","        return inputs + tf.cast(pos_encoding, tf.float32)\r\n","\r\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GTYoWx8LGlP5"},"source":["# **Attension**"]},{"cell_type":"markdown","metadata":{"id":"PfY1u5hXG_vx"},"source":["Attention computation"]},{"cell_type":"markdown","metadata":{"id":"8kW47ORiHAQp"},"source":["$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"]},{"cell_type":"code","metadata":{"id":"ijC6FPssGqjt","executionInfo":{"status":"ok","timestamp":1610374019705,"user_tz":-330,"elapsed":120616,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["def scaled_dot_product_attention(queries, keys, values, mask):\r\n","    product = tf.matmul(queries, keys, transpose_b=True)\r\n","    \r\n","    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\r\n","    scaled_product = product / tf.math.sqrt(keys_dim)\r\n","    \r\n","    if mask is not None:\r\n","        scaled_product += (mask * -1e9)\r\n","    \r\n","    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\r\n","    \r\n","    return attention"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZYWiCAxuHXYX"},"source":["Multi Head Attention Sublayer "]},{"cell_type":"code","metadata":{"id":"RdXoELqvHeKJ","executionInfo":{"status":"ok","timestamp":1610374019707,"user_tz":-330,"elapsed":120616,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["class MultiHeadAttention(layers.Layer):\r\n","    \r\n","    def __init__(self, nb_proj):\r\n","        super(MultiHeadAttention, self).__init__()\r\n","        self.nb_proj = nb_proj\r\n","        \r\n","    def build(self, input_shape):\r\n","        self.d_model = input_shape[-1]\r\n","        assert self.d_model % self.nb_proj == 0\r\n","        \r\n","        self.d_proj = self.d_model // self.nb_proj\r\n","        \r\n","        self.query_lin = layers.Dense(units=self.d_model)\r\n","        self.key_lin = layers.Dense(units=self.d_model)\r\n","        self.value_lin = layers.Dense(units=self.d_model)\r\n","        \r\n","        self.final_lin = layers.Dense(units=self.d_model)\r\n","        \r\n","    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\r\n","        shape = (batch_size,\r\n","                 -1,\r\n","                 self.nb_proj,\r\n","                 self.d_proj)\r\n","        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\r\n","        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\r\n","    \r\n","    def call(self, queries, keys, values, mask):\r\n","        batch_size = tf.shape(queries)[0]\r\n","        \r\n","        queries = self.query_lin(queries)\r\n","        keys = self.key_lin(keys)\r\n","        values = self.value_lin(values)\r\n","        \r\n","        queries = self.split_proj(queries, batch_size)\r\n","        keys = self.split_proj(keys, batch_size)\r\n","        values = self.split_proj(values, batch_size)\r\n","        \r\n","        attention = scaled_dot_product_attention(queries, keys, values, mask)\r\n","        \r\n","        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\r\n","        \r\n","        concat_attention = tf.reshape(attention,\r\n","                                      shape=(batch_size, -1, self.d_model))\r\n","        \r\n","        outputs = self.final_lin(concat_attention)\r\n","        \r\n","        return outputs"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nQ-nxui1H2Q5"},"source":["# **Encoder**"]},{"cell_type":"code","metadata":{"id":"uOunGQarG-Sq","executionInfo":{"status":"ok","timestamp":1610374019710,"user_tz":-330,"elapsed":120616,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["class EncoderLayer(layers.Layer):\r\n","    \r\n","    def __init__(self, FFN_units, nb_proj, dropout_rate):\r\n","        super(EncoderLayer, self).__init__()\r\n","        self.FFN_units = FFN_units\r\n","        self.nb_proj = nb_proj\r\n","        self.dropout_rate = dropout_rate\r\n","    \r\n","    def build(self, input_shape):\r\n","        self.d_model = input_shape[-1]\r\n","        \r\n","        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\r\n","        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\r\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\r\n","        \r\n","        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\r\n","        self.dense_2 = layers.Dense(units=self.d_model)\r\n","        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\r\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\r\n","        \r\n","    def call(self, inputs, mask, training):\r\n","        attention = self.multi_head_attention(inputs,\r\n","                                              inputs,\r\n","                                              inputs,\r\n","                                              mask)\r\n","        attention = self.dropout_1(attention, training=training)\r\n","        attention = self.norm_1(attention + inputs)\r\n","        \r\n","        outputs = self.dense_1(attention)\r\n","        outputs = self.dense_2(outputs)\r\n","        outputs = self.dropout_2(outputs, training=training)\r\n","        outputs = self.norm_2(outputs + attention)\r\n","        \r\n","        return outputs"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"16w7o7HNH7fg","executionInfo":{"status":"ok","timestamp":1610374019711,"user_tz":-330,"elapsed":120614,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["class Encoder(layers.Layer):\r\n","    \r\n","    def __init__(self,\r\n","                 nb_layers,\r\n","                 FFN_units,\r\n","                 nb_proj,\r\n","                 dropout_rate,\r\n","                 vocab_size,\r\n","                 d_model,\r\n","                 name=\"encoder\"):\r\n","        super(Encoder, self).__init__(name=name)\r\n","        self.nb_layers = nb_layers\r\n","        self.d_model = d_model\r\n","        \r\n","        self.embedding = layers.Embedding(vocab_size, d_model)\r\n","        self.pos_encoding = PositionalEncoding()\r\n","        self.dropout = layers.Dropout(rate=dropout_rate)\r\n","        self.enc_layers = [EncoderLayer(FFN_units,\r\n","                                        nb_proj,\r\n","                                        dropout_rate) \r\n","                           for _ in range(nb_layers)]\r\n","    \r\n","    def call(self, inputs, mask, training):\r\n","        outputs = self.embedding(inputs)\r\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n","        outputs = self.pos_encoding(outputs)\r\n","        outputs = self.dropout(outputs, training)\r\n","        \r\n","        for i in range(self.nb_layers):\r\n","            outputs = self.enc_layers[i](outputs, mask, training)\r\n","\r\n","        return outputs"],"execution_count":18,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UsCVOo-uJakT"},"source":["# **Decoder**"]},{"cell_type":"code","metadata":{"id":"zLQ3uKdDJdvA","executionInfo":{"status":"ok","timestamp":1610374019713,"user_tz":-330,"elapsed":120613,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["class DecoderLayer(layers.Layer):\r\n","    \r\n","    def __init__(self, FFN_units, nb_proj, dropout_rate):\r\n","        super(DecoderLayer, self).__init__()\r\n","        self.FFN_units = FFN_units\r\n","        self.nb_proj = nb_proj\r\n","        self.dropout_rate = dropout_rate\r\n","    \r\n","    def build(self, input_shape):\r\n","        self.d_model = input_shape[-1]\r\n","        \r\n","        # Self multi head attention\r\n","        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\r\n","        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\r\n","        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\r\n","        \r\n","        # Multi head attention combined with encoder output\r\n","        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\r\n","        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\r\n","        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\r\n","        \r\n","        # Feed foward\r\n","        self.dense_1 = layers.Dense(units=self.FFN_units,\r\n","                                    activation=\"relu\")\r\n","        self.dense_2 = layers.Dense(units=self.d_model)\r\n","        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\r\n","        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\r\n","        \r\n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\r\n","        attention = self.multi_head_attention_1(inputs,\r\n","                                                inputs,\r\n","                                                inputs,\r\n","                                                mask_1)\r\n","        attention = self.dropout_1(attention, training)\r\n","        attention = self.norm_1(attention + inputs)\r\n","        \r\n","        attention_2 = self.multi_head_attention_2(attention,\r\n","                                                  enc_outputs,\r\n","                                                  enc_outputs,\r\n","                                                  mask_2)\r\n","        attention_2 = self.dropout_2(attention_2, training)\r\n","        attention_2 = self.norm_2(attention_2 + attention)\r\n","        \r\n","        outputs = self.dense_1(attention_2)\r\n","        outputs = self.dense_2(outputs)\r\n","        outputs = self.dropout_3(outputs, training)\r\n","        outputs = self.norm_3(outputs + attention_2)\r\n","        \r\n","        return outputs"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"s2Wt-WvYJlbA","executionInfo":{"status":"ok","timestamp":1610374019715,"user_tz":-330,"elapsed":120611,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["class Decoder(layers.Layer):\r\n","    \r\n","    def __init__(self,\r\n","                 nb_layers,\r\n","                 FFN_units,\r\n","                 nb_proj,\r\n","                 dropout_rate,\r\n","                 vocab_size,\r\n","                 d_model,\r\n","                 name=\"decoder\"):\r\n","        super(Decoder, self).__init__(name=name)\r\n","        self.d_model = d_model\r\n","        self.nb_layers = nb_layers\r\n","        \r\n","        self.embedding = layers.Embedding(vocab_size, d_model)\r\n","        self.pos_encoding = PositionalEncoding()\r\n","        self.dropout = layers.Dropout(rate=dropout_rate)\r\n","        \r\n","        self.dec_layers = [DecoderLayer(FFN_units,\r\n","                                        nb_proj,\r\n","                                        dropout_rate) \r\n","                           for i in range(nb_layers)]\r\n","    \r\n","    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\r\n","        outputs = self.embedding(inputs)\r\n","        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\r\n","        outputs = self.pos_encoding(outputs)\r\n","        outputs = self.dropout(outputs, training)\r\n","        \r\n","        for i in range(self.nb_layers):\r\n","            outputs = self.dec_layers[i](outputs,\r\n","                                         enc_outputs,\r\n","                                         mask_1,\r\n","                                         mask_2,\r\n","                                         training)\r\n","\r\n","        return outputs"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d9hQMAjaJ-3V"},"source":["# **Transformer**"]},{"cell_type":"code","metadata":{"id":"krgy-1WAKCy5","executionInfo":{"status":"ok","timestamp":1610374019717,"user_tz":-330,"elapsed":120610,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["class Transformer(tf.keras.Model):\r\n","    \r\n","    def __init__(self,\r\n","                 vocab_size_enc,\r\n","                 vocab_size_dec,\r\n","                 d_model,\r\n","                 nb_layers,\r\n","                 FFN_units,\r\n","                 nb_proj,\r\n","                 dropout_rate,\r\n","                 name=\"transformer\"):\r\n","        super(Transformer, self).__init__(name=name)\r\n","        \r\n","        self.encoder = Encoder(nb_layers,\r\n","                               FFN_units,\r\n","                               nb_proj,\r\n","                               dropout_rate,\r\n","                               vocab_size_enc,\r\n","                               d_model)\r\n","        self.decoder = Decoder(nb_layers,\r\n","                               FFN_units,\r\n","                               nb_proj,\r\n","                               dropout_rate,\r\n","                               vocab_size_dec,\r\n","                               d_model)\r\n","        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\r\n","    \r\n","    def create_padding_mask(self, seq):\r\n","        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\r\n","        return mask[:, tf.newaxis, tf.newaxis, :]\r\n","\r\n","    def create_look_ahead_mask(self, seq):\r\n","        seq_len = tf.shape(seq)[1]\r\n","        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\r\n","        return look_ahead_mask\r\n","    \r\n","    def call(self, enc_inputs, dec_inputs, training):\r\n","        enc_mask = self.create_padding_mask(enc_inputs)\r\n","        dec_mask_1 = tf.maximum(\r\n","            self.create_padding_mask(dec_inputs),\r\n","            self.create_look_ahead_mask(dec_inputs)\r\n","        )\r\n","        dec_mask_2 = self.create_padding_mask(enc_inputs)\r\n","        \r\n","        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\r\n","        dec_outputs = self.decoder(dec_inputs,\r\n","                                   enc_outputs,\r\n","                                   dec_mask_1,\r\n","                                   dec_mask_2,\r\n","                                   training)\r\n","        \r\n","        outputs = self.last_linear(dec_outputs)\r\n","        \r\n","        return outputs"],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vCrgtwGRKMIz"},"source":["# **Training**"]},{"cell_type":"code","metadata":{"id":"ZnGlGgTzKOvf","executionInfo":{"status":"ok","timestamp":1610374019722,"user_tz":-330,"elapsed":120612,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["tf.keras.backend.clear_session()\r\n","\r\n","# Hyper-parameters\r\n","D_MODEL = 128 #512\r\n","NB_LAYERS = 4 #6\r\n","FFN_UNITS = 512 #2048\r\n","NB_PROJ = 8 # 8\r\n","DROPOUT_RATE = 0.1 # 0.1\r\n","\r\n","transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\r\n","                          vocab_size_dec=VOCAB_SIZE_BN,\r\n","                          d_model=D_MODEL,\r\n","                          nb_layers=NB_LAYERS,\r\n","                          FFN_units=FFN_UNITS,\r\n","                          nb_proj=NB_PROJ,\r\n","                          dropout_rate=DROPOUT_RATE)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8VVufAcKU9_","executionInfo":{"status":"ok","timestamp":1610374019724,"user_tz":-330,"elapsed":120611,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\r\n","                                                            reduction=\"none\")\r\n","\r\n","def loss_function(target, pred):\r\n","    mask = tf.math.logical_not(tf.math.equal(target, 0))\r\n","    loss_ = loss_object(target, pred)\r\n","    \r\n","    mask = tf.cast(mask, dtype=loss_.dtype)\r\n","    loss_ *= mask\r\n","    \r\n","    return tf.reduce_mean(loss_)\r\n","\r\n","train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\r\n","train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y6XBj6tsKc9l","executionInfo":{"status":"ok","timestamp":1610374019725,"user_tz":-330,"elapsed":120610,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\r\n","    \r\n","    def __init__(self, d_model, warmup_steps=4000):\r\n","        super(CustomSchedule, self).__init__()\r\n","        \r\n","        self.d_model = tf.cast(d_model, tf.float32)\r\n","        self.warmup_steps = warmup_steps\r\n","    \r\n","    def __call__(self, step):\r\n","        arg1 = tf.math.rsqrt(step)\r\n","        arg2 = step * (self.warmup_steps**-1.5)\r\n","        \r\n","        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\r\n","\r\n","leaning_rate = CustomSchedule(D_MODEL)\r\n","\r\n","optimizer = tf.keras.optimizers.Adam(leaning_rate,\r\n","                                     beta_1=0.9,\r\n","                                     beta_2=0.98,\r\n","                                     epsilon=1e-9)\r\n","        "],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dlIBqz-rKiNZ","executionInfo":{"status":"ok","timestamp":1610374022187,"user_tz":-330,"elapsed":123069,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}},"outputId":"41786bda-5b13-4a09-c823-fdcce4aa2051"},"source":["checkpoint_path = \"/content/drive/MyDrive/Colab Notebooks/Projects/transformer/ckpt/\"\r\n","\r\n","ckpt = tf.train.Checkpoint(transformer=transformer,\r\n","                           optimizer=optimizer)\r\n","\r\n","ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\r\n","\r\n","if ckpt_manager.latest_checkpoint:\r\n","    ckpt.restore(ckpt_manager.latest_checkpoint)\r\n","    print(\"Latest checkpoint restored!!\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Latest checkpoint restored!!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"h76GU7MFKnSU","executionInfo":{"status":"error","timestamp":1610384312717,"user_tz":-330,"elapsed":3936580,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}},"outputId":"ded2ee08-3a87-4167-fb17-26bc0c57c79d"},"source":["EPOCHS = 5 #10\r\n","for epoch in range(EPOCHS):\r\n","    print(\"Start of epoch {}\".format(epoch+1))\r\n","    start = time.time()\r\n","    \r\n","    train_loss.reset_states()\r\n","    train_accuracy.reset_states()\r\n","    \r\n","    for (batch, (enc_inputs, targets)) in enumerate(dataset):\r\n","        dec_inputs = targets[:, :-1]\r\n","        dec_outputs_real = targets[:, 1:]\r\n","        with tf.GradientTape() as tape:\r\n","            predictions = transformer(enc_inputs, dec_inputs, True)\r\n","            loss = loss_function(dec_outputs_real, predictions)\r\n","        \r\n","        gradients = tape.gradient(loss, transformer.trainable_variables)\r\n","        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\r\n","        \r\n","        train_loss(loss)\r\n","        train_accuracy(dec_outputs_real, predictions)\r\n","        \r\n","        if batch % 50 == 0:\r\n","            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\r\n","                epoch+1, batch, train_loss.result(), train_accuracy.result()))\r\n","            \r\n","    ckpt_save_path = ckpt_manager.save()\r\n","    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\r\n","                                                        ckpt_save_path))\r\n","    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["Start of epoch 1\n","Epoch 1 Batch 0 Loss 0.1525 Accuracy 0.5510\n","Epoch 1 Batch 50 Loss 0.1260 Accuracy 0.5696\n","Epoch 1 Batch 100 Loss 0.1072 Accuracy 0.5798\n","Epoch 1 Batch 150 Loss 0.0975 Accuracy 0.5827\n","Epoch 1 Batch 200 Loss 0.0911 Accuracy 0.5840\n","Epoch 1 Batch 250 Loss 0.0880 Accuracy 0.5865\n","Epoch 1 Batch 300 Loss 0.0858 Accuracy 0.5890\n","Epoch 1 Batch 350 Loss 0.0836 Accuracy 0.5905\n","Epoch 1 Batch 400 Loss 0.0820 Accuracy 0.5917\n","Epoch 1 Batch 450 Loss 0.0795 Accuracy 0.5935\n","Epoch 1 Batch 500 Loss 0.0772 Accuracy 0.5955\n","Epoch 1 Batch 550 Loss 0.0752 Accuracy 0.5970\n","Epoch 1 Batch 600 Loss 0.0743 Accuracy 0.5983\n","Epoch 1 Batch 650 Loss 0.0744 Accuracy 0.5994\n","Epoch 1 Batch 700 Loss 0.0750 Accuracy 0.6001\n","Epoch 1 Batch 750 Loss 0.0753 Accuracy 0.6005\n","Epoch 1 Batch 800 Loss 0.0754 Accuracy 0.5995\n","Epoch 1 Batch 850 Loss 0.0754 Accuracy 0.5985\n","Epoch 1 Batch 900 Loss 0.0757 Accuracy 0.5981\n","Epoch 1 Batch 950 Loss 0.0755 Accuracy 0.5973\n","Epoch 1 Batch 1000 Loss 0.0761 Accuracy 0.5959\n","Epoch 1 Batch 1050 Loss 0.0776 Accuracy 0.5947\n","Epoch 1 Batch 1100 Loss 0.0788 Accuracy 0.5933\n","Epoch 1 Batch 1150 Loss 0.0795 Accuracy 0.5922\n","Epoch 1 Batch 1200 Loss 0.0800 Accuracy 0.5912\n","Epoch 1 Batch 1250 Loss 0.0802 Accuracy 0.5902\n","Epoch 1 Batch 1300 Loss 0.0802 Accuracy 0.5896\n","Epoch 1 Batch 1350 Loss 0.0801 Accuracy 0.5894\n","Epoch 1 Batch 1400 Loss 0.0800 Accuracy 0.5886\n","Epoch 1 Batch 1450 Loss 0.0799 Accuracy 0.5879\n","Epoch 1 Batch 1500 Loss 0.0801 Accuracy 0.5873\n","Epoch 1 Batch 1550 Loss 0.0800 Accuracy 0.5867\n","Epoch 1 Batch 1600 Loss 0.0804 Accuracy 0.5863\n","Epoch 1 Batch 1650 Loss 0.0811 Accuracy 0.5857\n","Epoch 1 Batch 1700 Loss 0.0822 Accuracy 0.5849\n","Epoch 1 Batch 1750 Loss 0.0830 Accuracy 0.5840\n","Epoch 1 Batch 1800 Loss 0.0837 Accuracy 0.5827\n","Epoch 1 Batch 1850 Loss 0.0841 Accuracy 0.5817\n","Epoch 1 Batch 1900 Loss 0.0845 Accuracy 0.5810\n","Epoch 1 Batch 1950 Loss 0.0849 Accuracy 0.5803\n","Epoch 1 Batch 2000 Loss 0.0856 Accuracy 0.5794\n","Epoch 1 Batch 2050 Loss 0.0864 Accuracy 0.5783\n","Epoch 1 Batch 2100 Loss 0.0870 Accuracy 0.5772\n","Epoch 1 Batch 2150 Loss 0.0872 Accuracy 0.5759\n","Epoch 1 Batch 2200 Loss 0.0871 Accuracy 0.5746\n","Epoch 1 Batch 2250 Loss 0.0870 Accuracy 0.5732\n","Epoch 1 Batch 2300 Loss 0.0867 Accuracy 0.5718\n","Epoch 1 Batch 2350 Loss 0.0864 Accuracy 0.5705\n","Epoch 1 Batch 2400 Loss 0.0860 Accuracy 0.5692\n","Epoch 1 Batch 2450 Loss 0.0854 Accuracy 0.5678\n","Epoch 1 Batch 2500 Loss 0.0850 Accuracy 0.5665\n","Epoch 1 Batch 2550 Loss 0.0845 Accuracy 0.5656\n","Epoch 1 Batch 2600 Loss 0.0843 Accuracy 0.5648\n","Epoch 1 Batch 2650 Loss 0.0843 Accuracy 0.5641\n","Epoch 1 Batch 2700 Loss 0.0846 Accuracy 0.5631\n","Epoch 1 Batch 2750 Loss 0.0848 Accuracy 0.5620\n","Epoch 1 Batch 2800 Loss 0.0851 Accuracy 0.5613\n","Epoch 1 Batch 2850 Loss 0.0853 Accuracy 0.5607\n","Epoch 1 Batch 2900 Loss 0.0853 Accuracy 0.5604\n","Epoch 1 Batch 2950 Loss 0.0855 Accuracy 0.5601\n","Epoch 1 Batch 3000 Loss 0.0858 Accuracy 0.5599\n","Epoch 1 Batch 3050 Loss 0.0861 Accuracy 0.5596\n","Epoch 1 Batch 3100 Loss 0.0862 Accuracy 0.5595\n","Epoch 1 Batch 3150 Loss 0.0863 Accuracy 0.5593\n","Epoch 1 Batch 3200 Loss 0.0863 Accuracy 0.5593\n","Epoch 1 Batch 3250 Loss 0.0863 Accuracy 0.5593\n","Epoch 1 Batch 3300 Loss 0.0864 Accuracy 0.5594\n","Saving checkpoint for epoch 1 at /content/drive/MyDrive/Colab Notebooks/Projects/transformer/ckpt/ckpt-19\n","Time taken for 1 epoch: 3119.3935549259186 secs\n","\n","Start of epoch 2\n","Epoch 2 Batch 0 Loss 0.1686 Accuracy 0.6044\n","Epoch 2 Batch 50 Loss 0.1249 Accuracy 0.5795\n","Epoch 2 Batch 100 Loss 0.1036 Accuracy 0.5781\n","Epoch 2 Batch 150 Loss 0.0938 Accuracy 0.5818\n","Epoch 2 Batch 200 Loss 0.0884 Accuracy 0.5846\n","Epoch 2 Batch 250 Loss 0.0854 Accuracy 0.5861\n","Epoch 2 Batch 300 Loss 0.0840 Accuracy 0.5878\n","Epoch 2 Batch 350 Loss 0.0822 Accuracy 0.5894\n","Epoch 2 Batch 400 Loss 0.0800 Accuracy 0.5917\n","Epoch 2 Batch 450 Loss 0.0779 Accuracy 0.5927\n","Epoch 2 Batch 500 Loss 0.0756 Accuracy 0.5953\n","Epoch 2 Batch 550 Loss 0.0737 Accuracy 0.5969\n","Epoch 2 Batch 600 Loss 0.0724 Accuracy 0.5986\n","Epoch 2 Batch 650 Loss 0.0724 Accuracy 0.5996\n","Epoch 2 Batch 700 Loss 0.0724 Accuracy 0.6005\n","Epoch 2 Batch 750 Loss 0.0726 Accuracy 0.6008\n","Epoch 2 Batch 800 Loss 0.0728 Accuracy 0.6005\n","Epoch 2 Batch 850 Loss 0.0729 Accuracy 0.5997\n","Epoch 2 Batch 900 Loss 0.0732 Accuracy 0.5993\n","Epoch 2 Batch 950 Loss 0.0732 Accuracy 0.5986\n","Epoch 2 Batch 1000 Loss 0.0739 Accuracy 0.5971\n","Epoch 2 Batch 1050 Loss 0.0749 Accuracy 0.5958\n","Epoch 2 Batch 1100 Loss 0.0761 Accuracy 0.5944\n","Epoch 2 Batch 1150 Loss 0.0766 Accuracy 0.5932\n","Epoch 2 Batch 1200 Loss 0.0770 Accuracy 0.5921\n","Epoch 2 Batch 1250 Loss 0.0773 Accuracy 0.5909\n","Epoch 2 Batch 1300 Loss 0.0772 Accuracy 0.5905\n","Epoch 2 Batch 1350 Loss 0.0773 Accuracy 0.5898\n","Epoch 2 Batch 1400 Loss 0.0775 Accuracy 0.5891\n","Epoch 2 Batch 1450 Loss 0.0775 Accuracy 0.5886\n","Epoch 2 Batch 1500 Loss 0.0775 Accuracy 0.5879\n","Epoch 2 Batch 1550 Loss 0.0773 Accuracy 0.5875\n","Epoch 2 Batch 1600 Loss 0.0778 Accuracy 0.5870\n","Epoch 2 Batch 1650 Loss 0.0785 Accuracy 0.5865\n","Epoch 2 Batch 1700 Loss 0.0797 Accuracy 0.5857\n","Epoch 2 Batch 1750 Loss 0.0806 Accuracy 0.5849\n","Epoch 2 Batch 1800 Loss 0.0811 Accuracy 0.5839\n","Epoch 2 Batch 1850 Loss 0.0817 Accuracy 0.5827\n","Epoch 2 Batch 1900 Loss 0.0819 Accuracy 0.5818\n","Epoch 2 Batch 1950 Loss 0.0823 Accuracy 0.5812\n","Epoch 2 Batch 2000 Loss 0.0831 Accuracy 0.5802\n","Epoch 2 Batch 2050 Loss 0.0836 Accuracy 0.5792\n","Epoch 2 Batch 2100 Loss 0.0841 Accuracy 0.5779\n","Epoch 2 Batch 2150 Loss 0.0844 Accuracy 0.5767\n","Epoch 2 Batch 2200 Loss 0.0845 Accuracy 0.5753\n","Epoch 2 Batch 2250 Loss 0.0845 Accuracy 0.5739\n","Epoch 2 Batch 2300 Loss 0.0843 Accuracy 0.5727\n","Epoch 2 Batch 2350 Loss 0.0839 Accuracy 0.5712\n","Epoch 2 Batch 2400 Loss 0.0834 Accuracy 0.5699\n","Epoch 2 Batch 2450 Loss 0.0830 Accuracy 0.5687\n","Epoch 2 Batch 2500 Loss 0.0825 Accuracy 0.5674\n","Epoch 2 Batch 2550 Loss 0.0820 Accuracy 0.5664\n","Epoch 2 Batch 2600 Loss 0.0817 Accuracy 0.5657\n","Epoch 2 Batch 2650 Loss 0.0818 Accuracy 0.5651\n","Epoch 2 Batch 2700 Loss 0.0821 Accuracy 0.5640\n","Epoch 2 Batch 2750 Loss 0.0824 Accuracy 0.5629\n","Epoch 2 Batch 2800 Loss 0.0827 Accuracy 0.5622\n","Epoch 2 Batch 2850 Loss 0.0828 Accuracy 0.5616\n","Epoch 2 Batch 2900 Loss 0.0828 Accuracy 0.5612\n","Epoch 2 Batch 2950 Loss 0.0829 Accuracy 0.5609\n","Epoch 2 Batch 3000 Loss 0.0832 Accuracy 0.5608\n","Epoch 2 Batch 3050 Loss 0.0834 Accuracy 0.5606\n","Epoch 2 Batch 3100 Loss 0.0836 Accuracy 0.5604\n","Epoch 2 Batch 3150 Loss 0.0836 Accuracy 0.5602\n","Epoch 2 Batch 3200 Loss 0.0836 Accuracy 0.5602\n","Epoch 2 Batch 3250 Loss 0.0836 Accuracy 0.5601\n","Epoch 2 Batch 3300 Loss 0.0836 Accuracy 0.5601\n","Saving checkpoint for epoch 2 at /content/drive/MyDrive/Colab Notebooks/Projects/transformer/ckpt/ckpt-20\n","Time taken for 1 epoch: 3062.005530834198 secs\n","\n","Start of epoch 3\n","Epoch 3 Batch 0 Loss 0.0838 Accuracy 0.5954\n","Epoch 3 Batch 50 Loss 0.1156 Accuracy 0.5731\n","Epoch 3 Batch 100 Loss 0.0989 Accuracy 0.5802\n","Epoch 3 Batch 150 Loss 0.0894 Accuracy 0.5810\n","Epoch 3 Batch 200 Loss 0.0839 Accuracy 0.5847\n","Epoch 3 Batch 250 Loss 0.0809 Accuracy 0.5887\n","Epoch 3 Batch 300 Loss 0.0790 Accuracy 0.5907\n","Epoch 3 Batch 350 Loss 0.0776 Accuracy 0.5926\n","Epoch 3 Batch 400 Loss 0.0752 Accuracy 0.5936\n","Epoch 3 Batch 450 Loss 0.0732 Accuracy 0.5961\n","Epoch 3 Batch 500 Loss 0.0709 Accuracy 0.5982\n","Epoch 3 Batch 550 Loss 0.0694 Accuracy 0.5997\n","Epoch 3 Batch 600 Loss 0.0685 Accuracy 0.6007\n","Epoch 3 Batch 650 Loss 0.0687 Accuracy 0.6011\n","Epoch 3 Batch 700 Loss 0.0693 Accuracy 0.6020\n","Epoch 3 Batch 750 Loss 0.0695 Accuracy 0.6021\n","Epoch 3 Batch 800 Loss 0.0695 Accuracy 0.6016\n","Epoch 3 Batch 850 Loss 0.0699 Accuracy 0.6005\n","Epoch 3 Batch 900 Loss 0.0701 Accuracy 0.6002\n","Epoch 3 Batch 950 Loss 0.0701 Accuracy 0.5991\n","Epoch 3 Batch 1000 Loss 0.0707 Accuracy 0.5980\n","Epoch 3 Batch 1050 Loss 0.0719 Accuracy 0.5965\n","Epoch 3 Batch 1100 Loss 0.0733 Accuracy 0.5954\n","Epoch 3 Batch 1150 Loss 0.0740 Accuracy 0.5941\n","Epoch 3 Batch 1200 Loss 0.0743 Accuracy 0.5929\n","Epoch 3 Batch 1250 Loss 0.0742 Accuracy 0.5917\n","Epoch 3 Batch 1300 Loss 0.0745 Accuracy 0.5910\n","Epoch 3 Batch 1350 Loss 0.0745 Accuracy 0.5903\n","Epoch 3 Batch 1400 Loss 0.0744 Accuracy 0.5900\n","Epoch 3 Batch 1450 Loss 0.0745 Accuracy 0.5894\n","Epoch 3 Batch 1500 Loss 0.0746 Accuracy 0.5889\n","Epoch 3 Batch 1550 Loss 0.0745 Accuracy 0.5884\n","Epoch 3 Batch 1600 Loss 0.0748 Accuracy 0.5878\n","Epoch 3 Batch 1650 Loss 0.0756 Accuracy 0.5873\n","Epoch 3 Batch 1700 Loss 0.0768 Accuracy 0.5865\n","Epoch 3 Batch 1750 Loss 0.0777 Accuracy 0.5854\n","Epoch 3 Batch 1800 Loss 0.0784 Accuracy 0.5841\n","Epoch 3 Batch 1850 Loss 0.0789 Accuracy 0.5829\n","Epoch 3 Batch 1900 Loss 0.0792 Accuracy 0.5822\n","Epoch 3 Batch 1950 Loss 0.0795 Accuracy 0.5818\n","Epoch 3 Batch 2000 Loss 0.0802 Accuracy 0.5808\n","Epoch 3 Batch 2050 Loss 0.0808 Accuracy 0.5797\n","Epoch 3 Batch 2100 Loss 0.0811 Accuracy 0.5786\n","Epoch 3 Batch 2150 Loss 0.0813 Accuracy 0.5772\n","Epoch 3 Batch 2200 Loss 0.0813 Accuracy 0.5760\n","Epoch 3 Batch 2250 Loss 0.0813 Accuracy 0.5746\n","Epoch 3 Batch 2300 Loss 0.0811 Accuracy 0.5732\n","Epoch 3 Batch 2350 Loss 0.0808 Accuracy 0.5718\n","Epoch 3 Batch 2400 Loss 0.0804 Accuracy 0.5706\n","Epoch 3 Batch 2450 Loss 0.0800 Accuracy 0.5693\n","Epoch 3 Batch 2500 Loss 0.0794 Accuracy 0.5679\n","Epoch 3 Batch 2550 Loss 0.0790 Accuracy 0.5669\n","Epoch 3 Batch 2600 Loss 0.0789 Accuracy 0.5661\n","Epoch 3 Batch 2650 Loss 0.0789 Accuracy 0.5653\n","Epoch 3 Batch 2700 Loss 0.0792 Accuracy 0.5645\n","Epoch 3 Batch 2750 Loss 0.0794 Accuracy 0.5634\n","Epoch 3 Batch 2800 Loss 0.0797 Accuracy 0.5627\n","Epoch 3 Batch 2850 Loss 0.0799 Accuracy 0.5622\n","Epoch 3 Batch 2900 Loss 0.0799 Accuracy 0.5618\n","Epoch 3 Batch 2950 Loss 0.0800 Accuracy 0.5615\n","Epoch 3 Batch 3000 Loss 0.0803 Accuracy 0.5613\n","Epoch 3 Batch 3050 Loss 0.0806 Accuracy 0.5611\n","Epoch 3 Batch 3100 Loss 0.0806 Accuracy 0.5610\n","Epoch 3 Batch 3150 Loss 0.0807 Accuracy 0.5609\n","Epoch 3 Batch 3200 Loss 0.0807 Accuracy 0.5608\n","Epoch 3 Batch 3250 Loss 0.0808 Accuracy 0.5608\n","Epoch 3 Batch 3300 Loss 0.0808 Accuracy 0.5608\n","Saving checkpoint for epoch 3 at /content/drive/MyDrive/Colab Notebooks/Projects/transformer/ckpt/ckpt-21\n","Time taken for 1 epoch: 3073.9560916423798 secs\n","\n","Start of epoch 4\n","Epoch 4 Batch 0 Loss 0.1247 Accuracy 0.5781\n","Epoch 4 Batch 50 Loss 0.1119 Accuracy 0.5761\n","Epoch 4 Batch 100 Loss 0.0955 Accuracy 0.5772\n","Epoch 4 Batch 150 Loss 0.0866 Accuracy 0.5832\n","Epoch 4 Batch 200 Loss 0.0818 Accuracy 0.5857\n","Epoch 4 Batch 250 Loss 0.0792 Accuracy 0.5881\n","Epoch 4 Batch 300 Loss 0.0781 Accuracy 0.5907\n","Epoch 4 Batch 350 Loss 0.0761 Accuracy 0.5926\n","Epoch 4 Batch 400 Loss 0.0740 Accuracy 0.5945\n","Epoch 4 Batch 450 Loss 0.0720 Accuracy 0.5956\n","Epoch 4 Batch 500 Loss 0.0695 Accuracy 0.5979\n","Epoch 4 Batch 550 Loss 0.0680 Accuracy 0.5992\n","Epoch 4 Batch 600 Loss 0.0671 Accuracy 0.6007\n","Epoch 4 Batch 650 Loss 0.0671 Accuracy 0.6018\n","Epoch 4 Batch 700 Loss 0.0673 Accuracy 0.6026\n","Epoch 4 Batch 750 Loss 0.0676 Accuracy 0.6028\n","Epoch 4 Batch 800 Loss 0.0678 Accuracy 0.6021\n","Epoch 4 Batch 850 Loss 0.0680 Accuracy 0.6016\n","Epoch 4 Batch 900 Loss 0.0680 Accuracy 0.6014\n","Epoch 4 Batch 950 Loss 0.0681 Accuracy 0.6001\n","Epoch 4 Batch 1000 Loss 0.0688 Accuracy 0.5984\n","Epoch 4 Batch 1050 Loss 0.0699 Accuracy 0.5971\n","Epoch 4 Batch 1100 Loss 0.0709 Accuracy 0.5956\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-5db243f368b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdec_outputs_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MatMulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1693\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1695\u001b[0;31m     \u001b[0mgrad_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1696\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt_a\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mt_b\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1697\u001b[0m     \u001b[0mgrad_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmat_mul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmat_mul\u001b[0;34m(a, b, transpose_a, transpose_b, name)\u001b[0m\n\u001b[1;32m   5527\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m   5528\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MatMul\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_a\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranspose_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"transpose_b\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5529\u001b[0;31m         transpose_b)\n\u001b[0m\u001b[1;32m   5530\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5531\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"KrsY7N63KyAp"},"source":["# **Evaluating**"]},{"cell_type":"code","metadata":{"id":"LT04-4m9K1Hv","executionInfo":{"status":"ok","timestamp":1610384342970,"user_tz":-330,"elapsed":2333,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["def evaluate(inp_sentence):\r\n","    inp_sentence = \\\r\n","        [VOCAB_SIZE_EN-2] + tokenizer_en.encode(inp_sentence) + [VOCAB_SIZE_EN-1]\r\n","    enc_input = tf.expand_dims(inp_sentence, axis=0)\r\n","    \r\n","    output = tf.expand_dims([VOCAB_SIZE_BN-2], axis=0)\r\n","    \r\n","    for _ in range(MAX_LENGTH):\r\n","        predictions = transformer(enc_input, output, False)\r\n","        \r\n","        prediction = predictions[:, -1:, :]\r\n","        \r\n","        predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\r\n","        \r\n","        if predicted_id == VOCAB_SIZE_BN-1:\r\n","            return tf.squeeze(output, axis=0)\r\n","        \r\n","        output = tf.concat([output, predicted_id], axis=-1)\r\n","        \r\n","    return tf.squeeze(output, axis=0)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDXWjEdVK5jC","executionInfo":{"status":"ok","timestamp":1610384348455,"user_tz":-330,"elapsed":2449,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}}},"source":["def translate(sentence):\r\n","    output = evaluate(sentence).numpy()\r\n","    \r\n","    predicted_sentence = tokenizer_bn.decode(\r\n","        [i for i in output if i < VOCAB_SIZE_BN-2]\r\n","    )\r\n","    \r\n","    print(\"Input: {}\".format(sentence))\r\n","    print(\"Predicted translation: {}\".format(predicted_sentence))"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"De-v9eYcK-rq","executionInfo":{"status":"ok","timestamp":1610385391840,"user_tz":-330,"elapsed":3885,"user":{"displayName":"Dipen Pal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiF1_XP1uLGFp2I_ZkY88cdTKXu27kqp4AeTrqA=s64","userId":"03601592413101724605"}},"outputId":"4babb435-f9a7-4012-99c7-26d29cc1cfc6"},"source":["translate(\"This is a really powerful tool!\")\r\n","translate(\"This book is good\") "],"execution_count":38,"outputs":[{"output_type":"stream","text":["Input: This is a really powerful tool!\n","Predicted translation: এই পাসওয়ার্ড লেখা হবে\n","Input: This book is good\n","Predicted translation: এই ধরনের কাগোলাড\n"],"name":"stdout"}]}]}